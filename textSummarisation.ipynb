{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NaturalLanguageProcessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "15wzdK3KZTyWW3fOcqhxeRh1cgZ_4eiAz",
      "authorship_tag": "ABX9TyMGiYXOLuzp7b2SmS1FJAv8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjitk7/textSummarisationGithubIssues/blob/master/textSummarisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo6fM1Rwww_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "funO8pxLHSAL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0507f550-ab8f-4f63-89b9-750e1305da51"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eghZqX28xGPl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "03d3f8f5-163f-4ad2-db7b-3cac96127707"
      },
      "source": [
        "import logging\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "pd.set_option('display.max_colwidth', 500)\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.WARNING)\n",
        "print(tf.__version__)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2i2Kx1KzZwG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "701a78f6-e004-406c-9c58-9282055dc3c0"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/My Drive/natural-language-processing/github_issues.csv\")\n",
        "df"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>issue_url</th>\n",
              "      <th>issue_title</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"https://github.com/zhangyuanwei/node-images/i...</td>\n",
              "      <td>can't load the addon. issue to: https://github...</td>\n",
              "      <td>can't load the addon. issue to: https://github...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"https://github.com/Microsoft/pxt/issues/2543\"</td>\n",
              "      <td>hcl accessibility a11yblocking a11ymas mas4.2....</td>\n",
              "      <td>user experience: user who depends on screen re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"https://github.com/MatisiekPL/Czekolada/issue...</td>\n",
              "      <td>issue 1265: issue 1264: issue 1261: issue 1260...</td>\n",
              "      <td>┆attachments: &lt;a href= https:&amp; x2f;&amp; x2f;githu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"https://github.com/MatisiekPL/Czekolada/issue...</td>\n",
              "      <td>issue 1266: issue 1263: issue 1262: issue 1259...</td>\n",
              "      <td>gitlo = github x trello\\n---\\nthis board is no...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"https://github.com/MatisiekPL/Czekolada/issue...</td>\n",
              "      <td>issue 1288: issue 1285: issue 1284: issue 1281...</td>\n",
              "      <td>┆attachments: &lt;a href= https:&amp; x2f;&amp; x2f;githu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5332148</th>\n",
              "      <td>\"https://github.com/bayborodin/ror-full-3/issu...</td>\n",
              "      <td>создать модуль instancecounter, содержащий сле...</td>\n",
              "      <td>методы класса: - instances, который возвращает...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5332149</th>\n",
              "      <td>\"https://github.com/eclipse/paho.mqtt.java/iss...</td>\n",
              "      <td>at org.eclipse.paho.client.mqttv3.internal.cli...</td>\n",
              "      <td>- bug exists release version 1.1.0 master bran...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5332150</th>\n",
              "      <td>\"https://github.com/rzwitserloot/lombok/issues...</td>\n",
              "      <td>java.lang.linkageerror: loader constraint viol...</td>\n",
              "      <td>java.lang.linkageerror: loader constraint viol...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5332151</th>\n",
              "      <td>\"https://github.com/Gizra/productivity/issues/...</td>\n",
              "      <td>node : pdoexception: sqlstate 40001 : serializ...</td>\n",
              "      <td>view details in rollbar: https://rollbar.com/b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5332152</th>\n",
              "      <td>\"https://github.com/jacobmischka/coyote-grill/...</td>\n",
              "      <td>uncaught error: { error :{ errors : { domain :...</td>\n",
              "      <td>view details in rollbar: https://rollbar.com/j...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5332153 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 issue_url  ...                                               body\n",
              "0        \"https://github.com/zhangyuanwei/node-images/i...  ...  can't load the addon. issue to: https://github...\n",
              "1           \"https://github.com/Microsoft/pxt/issues/2543\"  ...  user experience: user who depends on screen re...\n",
              "2        \"https://github.com/MatisiekPL/Czekolada/issue...  ...  ┆attachments: <a href= https:& x2f;& x2f;githu...\n",
              "3        \"https://github.com/MatisiekPL/Czekolada/issue...  ...  gitlo = github x trello\\n---\\nthis board is no...\n",
              "4        \"https://github.com/MatisiekPL/Czekolada/issue...  ...  ┆attachments: <a href= https:& x2f;& x2f;githu...\n",
              "...                                                    ...  ...                                                ...\n",
              "5332148  \"https://github.com/bayborodin/ror-full-3/issu...  ...  методы класса: - instances, который возвращает...\n",
              "5332149  \"https://github.com/eclipse/paho.mqtt.java/iss...  ...  - bug exists release version 1.1.0 master bran...\n",
              "5332150  \"https://github.com/rzwitserloot/lombok/issues...  ...  java.lang.linkageerror: loader constraint viol...\n",
              "5332151  \"https://github.com/Gizra/productivity/issues/...  ...  view details in rollbar: https://rollbar.com/b...\n",
              "5332152  \"https://github.com/jacobmischka/coyote-grill/...  ...  view details in rollbar: https://rollbar.com/j...\n",
              "\n",
              "[5332153 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxpWNfjb5TO9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "a0369d58-1d94-4a8b-9cc0-ee4d6e650dea"
      },
      "source": [
        "#read in data sample 2M rows (for speed of tutorial)\n",
        "traindf, testdf = train_test_split(df.sample(n=200), \n",
        "                                   test_size=.10)\n",
        "\n",
        "\n",
        "#print out stats about shape of data\n",
        "print(f'Train: {traindf.shape[0]:,} rows {traindf.shape[1]:,} columns')\n",
        "print(f'Test: {testdf.shape[0]:,} rows {testdf.shape[1]:,} columns')\n",
        "\n",
        "# preview data\n",
        "traindf.head(3)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 180 rows 3 columns\n",
            "Test: 20 rows 3 columns\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>issue_url</th>\n",
              "      <th>issue_title</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1332149</th>\n",
              "      <td>\"https://github.com/fstpackage/fst/issues/102\"</td>\n",
              "      <td>your dataset needs at least one column.</td>\n",
              "      <td>i was interested to use this package to save large datasets. however, i am getting this error when trying to create a simple benchmark. r df &lt;- data.frame matrix runif 256 65536 , nrow = 256 file &lt;- tempfile fileext = .fst system.time fst::write.fst df, file &gt; error in fststore path, x, as.integer compress : your dataset needs at least one column. &gt; timing stopped at: 0.044 0 0.045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3387148</th>\n",
              "      <td>\"https://github.com/elixir-lang/elixir/issues/6172\"</td>\n",
              "      <td>elixir-lang.org blocked in russia</td>\n",
              "      <td>blocking date: may 10, 2016 status url: in registry domain: k-shkaf.ru ip: 104.24.104.149 | 104.24.105.149 | 104.28.10.110 | 104.28.11.110 ip elixir-lang.org 104.28.10.110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2711216</th>\n",
              "      <td>\"https://github.com/fuji97/pokemon-xenoverse-tracker/issues/139\"</td>\n",
              "      <td>mosse non prioritarie</td>\n",
              "      <td>mi capita spesso che un pkmn avversario più lento di me attacchi prima senza usare mosse prioritarie o rapidartigli</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                issue_url  ...                                                                                                                                                                                                                                                                                                                                                                                              body\n",
              "1332149                    \"https://github.com/fstpackage/fst/issues/102\"  ...  i was interested to use this package to save large datasets. however, i am getting this error when trying to create a simple benchmark. r df <- data.frame matrix runif 256 65536 , nrow = 256 file <- tempfile fileext = .fst system.time fst::write.fst df, file > error in fststore path, x, as.integer compress : your dataset needs at least one column. > timing stopped at: 0.044 0 0.045\n",
              "3387148               \"https://github.com/elixir-lang/elixir/issues/6172\"  ...                                                                                                                                                                                                                       blocking date: may 10, 2016 status url: in registry domain: k-shkaf.ru ip: 104.24.104.149 | 104.24.105.149 | 104.28.10.110 | 104.28.11.110 ip elixir-lang.org 104.28.10.110\n",
              "2711216  \"https://github.com/fuji97/pokemon-xenoverse-tracker/issues/139\"  ...                                                                                                                                                                                                                                                                               mi capita spesso che un pkmn avversario più lento di me attacchi prima senza usare mosse prioritarie o rapidartigli\n",
              "\n",
              "[3 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmLJciac8PPF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "396ffde6-b8d6-48f3-8902-3fe992e46002"
      },
      "source": [
        "train_body_raw = traindf.body.tolist()\n",
        "train_title_raw = traindf.issue_title.tolist()\n",
        "#preview output of first element\n",
        "train_body_raw[0]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'i was interested to use this package to save large datasets. however, i am getting this error when trying to create a simple benchmark. r df <- data.frame matrix runif 256 65536 , nrow = 256 file <- tempfile fileext = .fst system.time fst::write.fst df, file > error in fststore path, x, as.integer compress : your dataset needs at least one column. > timing stopped at: 0.044 0 0.045'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9dwQqxI99MM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q ktext"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voOOuY3o8ilV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "from ktext.preprocess import processor"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBbFBe6U-hdo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "36e8213c-b4f4-4bc7-e513-241e68200582"
      },
      "source": [
        "%%time\n",
        "# Clean, tokenize, and apply padding / truncating such that each document length = 70\n",
        "#  also, retain only the top 8,000 words in the vocabulary and set the remaining words\n",
        "#  to 1 which will become common index for rare words \n",
        "body_pp = processor(keep_n=8000, padding_maxlen=70)\n",
        "train_body_vecs = body_pp.fit_transform(train_body_raw)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:....tokenizing data\n",
            "WARNING:root:(1/2) done. 0 sec\n",
            "WARNING:root:....building corpus\n",
            "WARNING:root:(2/2) done. 0 sec\n",
            "WARNING:root:Finished parsing 180 documents.\n",
            "WARNING:root:...fit is finished, beginning transform\n",
            "WARNING:root:...padding data\n",
            "WARNING:root:done. 0 sec\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 38.5 ms, sys: 177 ms, total: 216 ms\n",
            "Wall time: 576 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewqvp1kC8wFH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "696f8bba-3fe2-4c01-bdf6-e8d7fc255a64"
      },
      "source": [
        "print('\\noriginal string:\\n', train_body_raw[0], '\\n')\n",
        "print('after pre-processing:\\n', train_body_vecs[0], '\\n')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "original string:\n",
            " i was interested to use this package to save large datasets. however, i am getting this error when trying to create a simple benchmark. r df <- data.frame matrix runif 256 65536 , nrow = 256 file <- tempfile fileext = .fst system.time fst::write.fst df, file > error in fststore path, x, as.integer compress : your dataset needs at least one column. > timing stopped at: 0.044 0 0.045 \n",
            "\n",
            "after pre-processing:\n",
            " [   0    0    0    0    6   86 1403    4   41   12   92    4  176  877\n",
            " 1404  626    6  177  475   12   24   21  201    4   87    5  178 1405\n",
            "  266  878   51  627  389 1406    2    2 1407    2   29 1408 1409  628\n",
            "  202  179  628  203  628  878   29   24    9 1410  390  267   20  391\n",
            " 1411   66 1412  180   30  879   57  629 1413 1414   30    2    2    2] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_1oV7r9AuP-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "9439c334-84f8-4f61-8f9f-fea83d094ec5"
      },
      "source": [
        "# Instantiate a text processor for the titles, with some different parameters\n",
        "#  append_indicators = True appends the tokens '_start_' and '_end_' to each\n",
        "#                      document\n",
        "#  padding = 'post' means that zero padding is appended to the end of the \n",
        "#             of the document (as opposed to the default which is 'pre')\n",
        "title_pp = processor(append_indicators=True, keep_n=4500, \n",
        "                     padding_maxlen=12, padding ='post')\n",
        "\n",
        "# process the title data\n",
        "train_title_vecs = title_pp.fit_transform(train_title_raw)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:....tokenizing data\n",
            "WARNING:root:(1/2) done. 0 sec\n",
            "WARNING:root:....building corpus\n",
            "WARNING:root:(2/2) done. 0 sec\n",
            "WARNING:root:Finished parsing 180 documents.\n",
            "WARNING:root:...fit is finished, beginning transform\n",
            "WARNING:root:...padding data\n",
            "WARNING:root:done. 0 sec\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-bUZzD_AyN_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a42843c8-654e-4de5-fcd4-6e1d7ada0e1e"
      },
      "source": [
        "print('\\noriginal string:\\n', train_title_raw[0])\n",
        "print('after pre-processing:\\n', train_title_vecs[0])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "original string:\n",
            " your dataset needs at least one column.\n",
            "after pre-processing:\n",
            " [  2 151 152  70  44 153  71 154   3   0   0   0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NHH5ySQA27a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dill as dpickle\n",
        "import numpy as np\n",
        "\n",
        "# Save the preprocessor\n",
        "with open('body_pp.dpkl', 'wb') as f:\n",
        "    dpickle.dump(body_pp, f)\n",
        "\n",
        "with open('title_pp.dpkl', 'wb') as f:\n",
        "    dpickle.dump(title_pp, f)\n",
        "\n",
        "# Save the processed data\n",
        "np.save('train_title_vecs.npy', train_title_vecs)\n",
        "np.save('train_body_vecs.npy', train_body_vecs)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luUfvoG2GkX9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "62ef063e-2c7f-4323-8a99-2983ed004188"
      },
      "source": [
        "!pip install -q annoy"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▌                               | 10kB 22.7MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |████                            | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 102kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 112kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 122kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 133kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 143kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 153kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 163kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 174kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 184kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 194kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 204kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 215kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 225kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 235kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 245kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 256kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 266kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 276kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 286kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 296kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 307kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 317kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 327kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 337kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 348kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 358kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 368kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 378kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 389kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 399kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 409kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 419kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 430kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 440kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 450kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 460kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 471kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 481kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 491kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 501kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 512kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 522kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 532kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 542kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 552kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 563kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 573kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 583kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 593kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 604kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 614kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 624kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 634kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645kB 2.6MB/s \n",
            "\u001b[?25h  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNyaCTpnA4_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp \"/content/drive/My Drive/natural-language-processing/seq2seq_utils.py\" .\n",
        "from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmvwDw_sC7jI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "497a03f6-5aca-46a5-fd73-9a2c8fafebcb"
      },
      "source": [
        "encoder_input_data, doc_length = load_encoder_inputs('train_body_vecs.npy')\n",
        "decoder_input_data, decoder_target_data = load_decoder_inputs('train_title_vecs.npy')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of encoder input: (180, 70)\n",
            "Shape of decoder input: (180, 11)\n",
            "Shape of decoder target: (180, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCEhGA44GLrr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "01b8e73c-035a-4a25-afed-12bab78386b0"
      },
      "source": [
        "num_encoder_tokens, body_pp = load_text_processor('body_pp.dpkl')\n",
        "num_decoder_tokens, title_pp = load_text_processor('title_pp.dpkl')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of vocabulary for body_pp.dpkl: 3,156\n",
            "Size of vocabulary for title_pp.dpkl: 760\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snRlFsR3HFWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, GRU, Dense, Embedding, Bidirectional, BatchNormalization\n",
        "from keras import optimizers"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2gpn01DHF3E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "613c30df-6377-4e90-86ed-83d532386271"
      },
      "source": [
        "#arbitrarly set latent dimension for embedding and hidden units\n",
        "latent_dim = 300\n",
        "\n",
        "##### Define Model Architecture ######\n",
        "\n",
        "########################\n",
        "#### Encoder Model ####\n",
        "encoder_inputs = Input(shape=(doc_length,), name='Encoder-Input')\n",
        "\n",
        "# Word embeding for encoder (ex: Issue Body)\n",
        "x = Embedding(num_encoder_tokens, latent_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
        "x = BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
        "\n",
        "# Intermediate GRU layer (optional)\n",
        "#x = GRU(latent_dim, name='Encoder-Intermediate-GRU', return_sequences=True)(x)\n",
        "#x = BatchNormalization(name='Encoder-Batchnorm-2')(x)\n",
        "\n",
        "# We do not need the `encoder_output` just the hidden state.\n",
        "_, state_h = GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)\n",
        "\n",
        "# Encapsulate the encoder as a separate entity so we can just \n",
        "#  encode without decoding if we want to.\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
        "\n",
        "seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
        "\n",
        "########################\n",
        "#### Decoder Model ####\n",
        "decoder_inputs = Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n",
        "\n",
        "# Word Embedding For Decoder (ex: Issue Titles)\n",
        "dec_emb = Embedding(num_decoder_tokens, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
        "dec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
        "\n",
        "# Set up the decoder, using `decoder_state_input` as initial state.\n",
        "decoder_gru = GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\n",
        "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\n",
        "x = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
        "\n",
        "# Dense layer for prediction\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Final-Output-Dense')\n",
        "decoder_outputs = decoder_dense(x)\n",
        "\n",
        "########################\n",
        "#### Seq2Seq Model ####\n",
        "\n",
        "#seq2seq_decoder_out = decoder_model([decoder_inputs, seq2seq_encoder_out])\n",
        "seq2seq_Model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "\n",
        "seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-b00aa5ead9f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m########################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#### Encoder Model ####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mencoder_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Encoder-Input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Word embeding for encoder (ex: Issue Body)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[0;34m(shape, batch_shape, name, dtype, sparse, tensor)\u001b[0m\n\u001b[1;32m    176\u001b[0m                              \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                              \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                              input_tensor=tensor)\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0;31m# Return tensor including _keras_shape and _keras_history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m# Note that in this case train_output and test_output are the same pointer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, batch_size, batch_input_shape, dtype, input_tensor, sparse, name)\u001b[0m\n\u001b[1;32m     85\u001b[0m                                          \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                                          \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                                          name=self.name)\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(shape, ndim, dtype, sparse, name)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_placeholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_learning_phase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0chbs8dHImR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}